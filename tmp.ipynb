{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14331378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\\settings.py\n",
      "```\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "class Settings:\n",
      "    \"\"\"Configuration settings for the RAG pipeline\"\"\"\n",
      "    \n",
      "    # Yandex Cloud credentials\n",
      "    FOLDER_ID = os.getenv(\"YANDEX_FOLDER_ID\")\n",
      "    API_KEY = os.getenv(\"YANDEX_API_KEY\")\n",
      "    \n",
      "    # Document processing\n",
      "    CHUNK_SIZE = 2000\n",
      "    CHUNK_OVERLAP = 250\n",
      "    \n",
      "    # Model settings\n",
      "    TEMPERATURE = 0.3\n",
      "    MAX_TOKENS = 8000\n",
      "\n",
      "    # Retrieval settings\n",
      "    SEARCH_KWARGS = {\"k\": 5}  # Number of documents to retrieve\n",
      "    SCORE_THRESHOLD = 0.7  # Minimum similarity score\n",
      "\n",
      "settings = Settings()\n",
      "```\n",
      "\n",
      "config\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "scripts\\run_pipeline.py\n",
      "```\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Main script to run the RAG pipeline\n",
      "\"\"\"\n",
      "\n",
      "import sys\n",
      "import os\n",
      "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
      "\n",
      "from src.data.document_loader import DocumentLoader\n",
      "from src.processing.text_splitter import TextSplitter\n",
      "from src.processing.embeddings import EmbeddingManager\n",
      "from src.retrieval.vector_store import VectorStoreManager\n",
      "from src.generation.qa_chain import QASystem\n",
      "from src.utils.helpers import setup_logging, format_sources\n",
      "from config.settings import settings\n",
      "\n",
      "def main(questions):\n",
      "    setup_logging()\n",
      "    \n",
      "    try:\n",
      "        # Initialize document loader for Consultant Plus\n",
      "        print(\"Initializing Consultant Plus loader...\")\n",
      "        loader = DocumentLoader(use_consultant_plus=True)\n",
      "        \n",
      "        for question in questions:\n",
      "            print(f\"\\n{'='*50}\")\n",
      "            print(f\"Question: {question}\")\n",
      "            print(f\"{'='*50}\")\n",
      "            \n",
      "            # 1. Load documents from Consultant Plus based on question\n",
      "            print(\"Searching Consultant Plus...\")\n",
      "            documents = loader.load_documents_from_query(question)\n",
      "            \n",
      "            if not documents:\n",
      "                print(\"No documents found for this query\")\n",
      "                continue\n",
      "                \n",
      "            print(f\"Found {len(documents)} relevant documents\")\n",
      "            \n",
      "            # 2. Split documents\n",
      "            print(\"Processing documents...\")\n",
      "            splitter = TextSplitter()\n",
      "            chunks = splitter.split_documents(documents)\n",
      "            print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
      "            \n",
      "            # 3. Create embeddings and vector store\n",
      "            print(\"Creating embeddings and vector store...\")\n",
      "            embedding_manager = EmbeddingManager()\n",
      "            embeddings = embedding_manager.get_embeddings()\n",
      "            \n",
      "            vector_manager = VectorStoreManager(embeddings)\n",
      "            vector_store = vector_manager.create_vector_store(chunks)\n",
      "            \n",
      "            # 4. Create QA system\n",
      "            print(\"Initializing QA system...\")\n",
      "            retriever = vector_manager.get_retriever()\n",
      "            qa_system = QASystem(retriever)\n",
      "            \n",
      "            # 5. Make query\n",
      "            system_prompt = \"Ты специалист по российскому праву. Подробно объясни ответ, указав ссылки на источники\"\n",
      "            \n",
      "            result = qa_system.query(question, system_prompt)\n",
      "            print(f\"Answer: {result['answer']}\")\n",
      "            print(f\"\\nSources:\\n{format_sources(result['source_documents'])}\")\n",
      "            \n",
      "    except Exception as e:\n",
      "        print(f\"Error in pipeline: {e}\")\n",
      "        raise\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main(\n",
      "        questions=[\n",
      "            \"трудовой кодекс отпуск\",\n",
      "            # \"налоговые вычеты для физических лиц\",\n",
      "            # Add your questions here\n",
      "        ],\n",
      "    )\n",
      "```\n",
      "\n",
      "src\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "src\\data\\consultant_plus_loader.py\n",
      "```\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "from langchain.schema import Document\n",
      "from typing import List, Optional\n",
      "import urllib.parse\n",
      "import logging\n",
      "import time\n",
      "import re\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "class ConsultantPlusLoader:\n",
      "    \"\"\"Loader for Консультант Плюс search results and document content\"\"\"\n",
      "    \n",
      "    def __init__(self, max_results: int = 5):\n",
      "        self.max_results = max_results\n",
      "        self.base_url = \"https://www.consultant.ru\"\n",
      "        self.search_url = \"https://www.consultant.ru/search/\"\n",
      "        self.session = requests.Session()\n",
      "        self.session.headers.update({\n",
      "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
      "        })\n",
      "    \n",
      "    def search_documents(self, query: str) -> List[dict]:\n",
      "        \"\"\"Search documents on Консультант Плюс and return results with metadata\"\"\"\n",
      "        try:\n",
      "            encoded_query = urllib.parse.quote(query.encode('utf-8'))\n",
      "            url = f\"{self.search_url}?q={encoded_query}\"\n",
      "            \n",
      "            logger.info(f\"Fetching search results from: {url}\")\n",
      "            response = self.session.get(url, timeout=30)\n",
      "            response.raise_for_status()\n",
      "            \n",
      "            # Use lxml parser if available, otherwise use html.parser\n",
      "            try:\n",
      "                soup = BeautifulSoup(response.content, 'lxml')\n",
      "            except:\n",
      "                soup = BeautifulSoup(response.content, 'html.parser')\n",
      "            \n",
      "            results = []\n",
      "            \n",
      "            # Find search results\n",
      "            search_results = soup.find('ol', class_='search-results')\n",
      "            if not search_results:\n",
      "                logger.warning(\"No search results found on page\")\n",
      "                return results\n",
      "            \n",
      "            items = search_results.find_all('li', class_=re.compile('search-results__item'))[:self.max_results]\n",
      "            logger.info(f\"Found {len(items)} search result items\")\n",
      "            \n",
      "            for i, item in enumerate(items):\n",
      "                try:\n",
      "                    # Skip revoked documents and unavailable ones\n",
      "                    item_classes = item.get('class', [])\n",
      "                    if 'search-results__item_revoke' in item_classes:\n",
      "                        logger.debug(f\"Skipping revoked document at position {i}\")\n",
      "                        continue\n",
      "                    \n",
      "                    # Check availability\n",
      "                    availability_icon = item.find('i', class_='search-results__icon')\n",
      "                    if availability_icon and 'недоступен' in availability_icon.get('title', ''):\n",
      "                        logger.debug(f\"Skipping unavailable document at position {i}\")\n",
      "                        continue\n",
      "                    \n",
      "                    link = item.find('a', class_='search-results__link')\n",
      "                    if not link:\n",
      "                        continue\n",
      "                    \n",
      "                    href = link.get('href')\n",
      "                    if not href:\n",
      "                        continue\n",
      "                    \n",
      "                    # Make absolute URL\n",
      "                    if href.startswith('//'):\n",
      "                        doc_url = 'https:' + href\n",
      "                    elif href.startswith('/'):\n",
      "                        doc_url = self.base_url + href\n",
      "                    else:\n",
      "                        doc_url = href\n",
      "                    \n",
      "                    # Extract title\n",
      "                    title_elem = link.find('p', class_='search-results__link-inherit')\n",
      "                    title = title_elem.get_text(strip=True) if title_elem else \"No title\"\n",
      "                    \n",
      "                    # Extract description\n",
      "                    desc_elem = link.find('p', class_='search-results__descr')\n",
      "                    description = desc_elem.get_text(strip=True) if desc_elem else \"\"\n",
      "                    \n",
      "                    # Extract text info\n",
      "                    text_elem = link.find('p', class_='search-results__text')\n",
      "                    text_info = text_elem.get_text(strip=True) if text_elem else \"\"\n",
      "                    \n",
      "                    results.append({\n",
      "                        'url': doc_url,\n",
      "                        'title': title,\n",
      "                        'description': description,\n",
      "                        'text_info': text_info,\n",
      "                        'relevance_score': len(results) + 1,\n",
      "                        'position': i + 1\n",
      "                    })\n",
      "                    \n",
      "                    logger.debug(f\"Added result: {title[:50]}...\")\n",
      "                    \n",
      "                except Exception as e:\n",
      "                    logger.warning(f\"Error processing search result {i}: {e}\")\n",
      "                    continue\n",
      "            \n",
      "            logger.info(f\"Processed {len(results)} valid search results\")\n",
      "            return results\n",
      "            \n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error searching Consultant Plus: {e}\")\n",
      "            return []\n",
      "    \n",
      "    def load_document_content(self, url: str) -> Optional[str]:\n",
      "        \"\"\"Load and extract text content from a document URL\"\"\"\n",
      "        try:\n",
      "            logger.debug(f\"Loading document content from: {url}\")\n",
      "            response = self.session.get(url, timeout=30)\n",
      "            response.raise_for_status()\n",
      "            \n",
      "            # Check if it's XML content\n",
      "            content_type = response.headers.get('content-type', '').lower()\n",
      "            if 'xml' in content_type or url.endswith('.cgi'):\n",
      "                # Use XML parser for XML content\n",
      "                try:\n",
      "                    soup = BeautifulSoup(response.content, 'xml')\n",
      "                except:\n",
      "                    soup = BeautifulSoup(response.content, 'lxml')\n",
      "            else:\n",
      "                # Use HTML parser for regular pages\n",
      "                try:\n",
      "                    soup = BeautifulSoup(response.content, 'lxml')\n",
      "                except:\n",
      "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
      "            \n",
      "            # Remove unwanted elements\n",
      "            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):\n",
      "                element.decompose()\n",
      "            \n",
      "            # Try to find main content areas - Консультант Плюс specific selectors\n",
      "            content_selectors = [\n",
      "                '.document-page__text',\n",
      "                '.text',\n",
      "                'article',\n",
      "                'main',\n",
      "                '.content',\n",
      "                '.document-text',\n",
      "                '#content',\n",
      "                '.law-content'\n",
      "            ]\n",
      "            \n",
      "            content = None\n",
      "            for selector in content_selectors:\n",
      "                content_elem = soup.select_one(selector)\n",
      "                if content_elem:\n",
      "                    content = content_elem.get_text(separator=' ', strip=True)\n",
      "                    if len(content) > 100:  # Only use if we got substantial content\n",
      "                        break\n",
      "            \n",
      "            # If no specific content found, get body text but clean it up\n",
      "            if not content or len(content) < 100:\n",
      "                body = soup.find('body')\n",
      "                if body:\n",
      "                    # Remove empty elements and navigation\n",
      "                    for element in body.find_all(['div', 'span', 'p']):\n",
      "                        if len(element.get_text(strip=True)) < 10:\n",
      "                            element.decompose()\n",
      "                    content = body.get_text(separator=' ', strip=True)\n",
      "            \n",
      "            # Clean up the text - remove extra whitespace\n",
      "            if content:\n",
      "                content = re.sub(r'\\s+', ' ', content)\n",
      "                content = content.strip()\n",
      "                \n",
      "                # Basic validation - if content is too short, it might be an error page\n",
      "                if len(content) < 50:\n",
      "                    logger.warning(f\"Document content too short ({len(content)} chars), may be invalid\")\n",
      "                    return None\n",
      "                \n",
      "                logger.debug(f\"Extracted {len(content)} characters from document\")\n",
      "            \n",
      "            return content\n",
      "            \n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error loading document from {url}: {e}\")\n",
      "            return None\n",
      "    \n",
      "    def load_documents(self, query: str) -> List[Document]:\n",
      "        \"\"\"Main method to load documents based on search query\"\"\"\n",
      "        logger.info(f\"Searching Consultant Plus for: {query}\")\n",
      "        \n",
      "        search_results = self.search_documents(query)\n",
      "        if not search_results:\n",
      "            logger.warning(\"No search results found\")\n",
      "            return []\n",
      "        \n",
      "        documents = []\n",
      "        for i, result in enumerate(search_results):\n",
      "            logger.info(f\"Loading document {i+1}/{len(search_results)}: {result['title'][:80]}...\")\n",
      "            \n",
      "            content = self.load_document_content(result['url'])\n",
      "            if content:\n",
      "                # Create LangChain Document with metadata\n",
      "                metadata = {\n",
      "                    'source': result['url'],\n",
      "                    'title': result['title'],\n",
      "                    'description': result['description'],\n",
      "                    'text_info': result['text_info'],\n",
      "                    'relevance_score': result['relevance_score'],\n",
      "                    'position': result['position']\n",
      "                }\n",
      "                \n",
      "                document = Document(\n",
      "                    page_content=content,\n",
      "                    metadata=metadata\n",
      "                )\n",
      "                documents.append(document)\n",
      "                logger.info(f\"Successfully loaded document {i+1}\")\n",
      "            else:\n",
      "                logger.warning(f\"Failed to load content for document {i+1}\")\n",
      "            \n",
      "            # Be respectful with requests\n",
      "            time.sleep(1.5)\n",
      "        \n",
      "        logger.info(f\"Loaded {len(documents)} documents from Consultant Plus\")\n",
      "        return documents\n",
      "```\n",
      "\n",
      "src\\data\\document_loader.py\n",
      "```\n",
      "import os\n",
      "from typing import List\n",
      "from langchain.schema import Document\n",
      "\n",
      "# PDF functionality (commented but kept)\n",
      "\"\"\"\n",
      "from langchain_community.document_loaders import (\n",
      "    PyPDFLoader,\n",
      "    DirectoryLoader,\n",
      "    TextLoader,\n",
      "    UnstructuredFileLoader\n",
      ")\n",
      "\"\"\"\n",
      "\n",
      "from .consultant_plus_loader import ConsultantPlusLoader\n",
      "\n",
      "class DocumentLoader:\n",
      "    \"\"\"Main document loader that can use both PDFs and Consultant Plus\"\"\"\n",
      "    \n",
      "    def __init__(self, source: str = None, use_consultant_plus: bool = True):\n",
      "        self.source = source\n",
      "        self.use_consultant_plus = use_consultant_plus\n",
      "        self.consultant_loader = ConsultantPlusLoader()\n",
      "    \n",
      "    def load_documents(self, query: str = None) -> List[Document]:\n",
      "        \"\"\"Load documents from Consultant Plus based on query\"\"\"\n",
      "        if self.use_consultant_plus and query:\n",
      "            return self.consultant_loader.load_documents(query)\n",
      "        \n",
      "        # PDF functionality (commented but kept for reference)\n",
      "        \"\"\"\n",
      "        if not self.source or not os.path.exists(self.source):\n",
      "            raise ValueError(f\"Document path does not exist: {self.source}\")\n",
      "        \n",
      "        if os.path.isdir(self.source):\n",
      "            loader = DirectoryLoader(\n",
      "                self.source,\n",
      "                glob=\"**/*.pdf\",\n",
      "                loader_cls=PyPDFLoader\n",
      "            )\n",
      "        else:\n",
      "            if self.source.endswith('.pdf'):\n",
      "                loader = PyPDFLoader(self.source)\n",
      "            else:\n",
      "                loader = UnstructuredFileLoader(self.source)\n",
      "        \n",
      "        return loader.load()\n",
      "        \"\"\"\n",
      "        \n",
      "        raise ValueError(\"Either provide a query for Consultant Plus or enable PDF loading with valid source\")\n",
      "\n",
      "    def load_documents_from_query(self, query: str) -> List[Document]:\n",
      "        \"\"\"Convenience method to load documents from Consultant Plus using query\"\"\"\n",
      "        return self.consultant_loader.load_documents(query)\n",
      "```\n",
      "\n",
      "src\\data\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "src\\generation\\qa_chain.py\n",
      "```\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain_community.llms import YandexGPT\n",
      "from config.settings import settings\n",
      "import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "class QASystem:\n",
      "    \"\"\"Question-Answering system with RAG\"\"\"\n",
      "    \n",
      "    def __init__(self, retriever, llm=None):\n",
      "        self.retriever = retriever\n",
      "        self.llm = llm or self._create_llm()\n",
      "        self.qa_chain = self._create_qa_chain()\n",
      "    \n",
      "    def _create_llm(self):\n",
      "        \"\"\"Create YandexGPT LLM instance\"\"\"\n",
      "        return YandexGPT(\n",
      "            folder_id=settings.FOLDER_ID,\n",
      "            api_key=settings.API_KEY,\n",
      "            temperature=settings.TEMPERATURE,\n",
      "            max_tokens=settings.MAX_TOKENS\n",
      "        )\n",
      "    \n",
      "    def _create_qa_chain(self):\n",
      "        \"\"\"Create QA chain with custom prompt\"\"\"\n",
      "        prompt_template = \"\"\"Ты специалист по российскому праву. \n",
      "Используй предоставленный контекст из системы Консультант Плюс, чтобы подробно ответить на вопрос. \n",
      "Если в контексте нет достаточной информации, используй свои знания, но укажи это.\n",
      "\n",
      "Контекст:\n",
      "{context}\n",
      "\n",
      "Вопрос: {question}\n",
      "\n",
      "Требования к ответу:\n",
      "1. Подробно объясни ответ со ссылками на законодательство\n",
      "2. Укажи конкретные статьи и нормативные акты\n",
      "3. Будь точным и структурированным\n",
      "4. Укажи источники информации\n",
      "\n",
      "Ответ:\"\"\"\n",
      "        \n",
      "        PROMPT = PromptTemplate(\n",
      "            template=prompt_template, input_variables=[\"context\", \"question\"]\n",
      "        )\n",
      "        \n",
      "        return RetrievalQA.from_chain_type(\n",
      "            llm=self.llm,\n",
      "            chain_type=\"stuff\",\n",
      "            retriever=self.retriever,\n",
      "            chain_type_kwargs={\"prompt\": PROMPT},\n",
      "            return_source_documents=True\n",
      "        )\n",
      "    \n",
      "    def query(self, question: str, system_prompt: str = None) -> dict:\n",
      "        \"\"\"Query the QA system\"\"\"\n",
      "        try:\n",
      "            if system_prompt:\n",
      "                full_question = f\"{system_prompt}\\n\\nВопрос: {question}\"\n",
      "            else:\n",
      "                full_question = question\n",
      "            \n",
      "            result = self.qa_chain({\"query\": full_question})\n",
      "            return {\n",
      "                \"answer\": result[\"result\"],\n",
      "                \"source_documents\": result.get(\"source_documents\", []),\n",
      "                \"question\": question\n",
      "            }\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error querying QA system: {e}\")\n",
      "            raise\n",
      "```\n",
      "\n",
      "src\\generation\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "src\\processing\\embeddings.py\n",
      "```\n",
      "from langchain_community.embeddings.yandex import YandexGPTEmbeddings\n",
      "from config.settings import settings\n",
      "import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "class EmbeddingManager:\n",
      "    \"\"\"Manages embedding generation and caching\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.embeddings = YandexGPTEmbeddings(\n",
      "            folder_id=settings.FOLDER_ID,\n",
      "            api_key=settings.API_KEY\n",
      "        )\n",
      "    \n",
      "    def get_embeddings(self):\n",
      "        \"\"\"Get embeddings instance\"\"\"\n",
      "        return self.embeddings\n",
      "    \n",
      "    def embed_documents(self, documents: list):\n",
      "        \"\"\"Embed a list of documents\"\"\"\n",
      "        try:\n",
      "            return self.embeddings.embed_documents([doc.page_content for doc in documents])\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error embedding documents: {e}\")\n",
      "            raise e\n",
      "```\n",
      "\n",
      "src\\processing\\text_splitter.py\n",
      "```\n",
      "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
      "from langchain.schema import Document\n",
      "from typing import List\n",
      "from config.settings import settings\n",
      "\n",
      "class TextSplitter:\n",
      "    \"\"\"Handles document splitting with various strategies\"\"\"\n",
      "    \n",
      "    def __init__(self, chunk_size: int = None, chunk_overlap: int = None):\n",
      "        self.chunk_size = chunk_size or settings.CHUNK_SIZE\n",
      "        self.chunk_overlap = chunk_overlap or settings.CHUNK_OVERLAP\n",
      "    \n",
      "    def split_documents(self, documents: List[Document], method: str = \"recursive\") -> List[Document]:\n",
      "        \"\"\"Split documents into chunks\"\"\"\n",
      "        if method == \"recursive\":\n",
      "            splitter = RecursiveCharacterTextSplitter(\n",
      "                chunk_size=self.chunk_size,\n",
      "                chunk_overlap=self.chunk_overlap,\n",
      "                separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
      "            )\n",
      "        else:\n",
      "            splitter = CharacterTextSplitter(\n",
      "                chunk_size=self.chunk_size,\n",
      "                chunk_overlap=self.chunk_overlap\n",
      "            )\n",
      "        \n",
      "        return splitter.split_documents(documents)\n",
      "```\n",
      "\n",
      "src\\processing\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "src\\retrieval\\retriever.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "src\\retrieval\\vector_store.py\n",
      "```\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.schema import Document\n",
      "from typing import List, Optional\n",
      "import logging\n",
      "from config.settings import settings\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "class VectorStoreManager:\n",
      "    \"\"\"Manages FAISS vector store operations\"\"\"\n",
      "    \n",
      "    def __init__(self, embeddings):\n",
      "        self.embeddings = embeddings\n",
      "        self.vector_store = None\n",
      "    \n",
      "    def create_vector_store(self, documents: List[Document]):\n",
      "        \"\"\"Create FAISS vector store from documents\"\"\"\n",
      "        try:\n",
      "            self.vector_store = FAISS.from_documents(documents, self.embeddings)\n",
      "            logger.info(f\"Created vector store with {len(documents)} documents\")\n",
      "            return self.vector_store\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error creating vector store: {e}\")\n",
      "            raise\n",
      "    \n",
      "    def save_vector_store(self, path: str):\n",
      "        \"\"\"Save vector store to disk\"\"\"\n",
      "        if self.vector_store:\n",
      "            self.vector_store.save_local(path)\n",
      "            logger.info(f\"Vector store saved to {path}\")\n",
      "    \n",
      "    def load_vector_store(self, path: str):\n",
      "        \"\"\"Load vector store from disk\"\"\"\n",
      "        try:\n",
      "            self.vector_store = FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization=True)\n",
      "            logger.info(f\"Vector store loaded from {path}\")\n",
      "            return self.vector_store\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error loading vector store: {e}\")\n",
      "            raise\n",
      "    \n",
      "    def get_retriever(self, search_type: str = \"similarity\", **kwargs):\n",
      "        \"\"\"Get retriever from vector store\"\"\"\n",
      "        if not self.vector_store:\n",
      "            raise ValueError(\"Vector store not initialized\")\n",
      "        \n",
      "        search_kwargs = {**settings.SEARCH_KWARGS, **kwargs}\n",
      "        return self.vector_store.as_retriever(\n",
      "            search_type=search_type,\n",
      "            search_kwargs=search_kwargs\n",
      "        )\n",
      "```\n",
      "\n",
      "src\\retrieval\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "src\\utils\\helpers.py\n",
      "```\n",
      "import logging\n",
      "from typing import List\n",
      "\n",
      "def setup_logging(level=logging.INFO):\n",
      "    \"\"\"Setup logging configuration\"\"\"\n",
      "    logging.basicConfig(\n",
      "        level=level,\n",
      "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
      "        handlers=[\n",
      "            logging.StreamHandler(),\n",
      "            logging.FileHandler('rag_pipeline.log')\n",
      "        ]\n",
      "    )\n",
      "\n",
      "def format_sources(source_documents: List) -> str:\n",
      "    \"\"\"Format source documents for display\"\"\"\n",
      "    if not source_documents:\n",
      "        return \"No sources found\"\n",
      "    \n",
      "    sources = []\n",
      "    for i, doc in enumerate(source_documents, 1):\n",
      "        source_info = f\"Source {i}:\"\n",
      "        if hasattr(doc, 'metadata') and 'source' in doc.metadata:\n",
      "            source_info += f\" {doc.metadata['source']}\"\n",
      "        if hasattr(doc, 'metadata') and 'page' in doc.metadata:\n",
      "            source_info += f\" (Page {doc.metadata['page']})\"\n",
      "        sources.append(source_info)\n",
      "    \n",
      "    return \"\\n\".join(sources)\n",
      "```\n",
      "\n",
      "src\\utils\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def print_repo_contents(repo_path=\".\"):\n",
    "    \"\"\"Print all file contents in the repository\"\"\"\n",
    "    repo_path = Path(repo_path)\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            if file.startswith('.') or file.endswith('.pyc') or file.endswith('.md') or file.endswith('.txt') or file.endswith('.ipynb') or file.endswith('.log'):\n",
    "                continue\n",
    "            print(f\"{file_path}\")\n",
    "            print(\"```\")\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                print(f.read())\n",
    "            print(\"```\\n\")\n",
    "\n",
    "print_repo_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3298c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 18:30:55,974 - src.data.consultant_plus_loader - INFO - Searching Consultant Plus for: Сколько дней отпуска в год я могу взять?\n",
      "2025-11-30 18:30:55,975 - src.data.consultant_plus_loader - INFO - Fetching search results from: https://www.consultant.ru/search/?q=%D0%A1%D0%BA%D0%BE%D0%BB%D1%8C%D0%BA%D0%BE%20%D0%B4%D0%BD%D0%B5%D0%B9%20%D0%BE%D1%82%D0%BF%D1%83%D1%81%D0%BA%D0%B0%20%D0%B2%20%D0%B3%D0%BE%D0%B4%20%D1%8F%20%D0%BC%D0%BE%D0%B3%D1%83%20%D0%B2%D0%B7%D1%8F%D1%82%D1%8C%3F\n",
      "2025-11-30 18:30:56,114 - src.data.consultant_plus_loader - INFO - Found 5 search result items\n",
      "2025-11-30 18:30:56,116 - src.data.consultant_plus_loader - INFO - Processed 4 valid search results\n",
      "2025-11-30 18:30:56,117 - src.data.consultant_plus_loader - INFO - Loading document 1/4: Несовершеннолетний работник был принят на работу с 1 июля 2012 г. 31 августа 201...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Consultant Plus loader...\n",
      "\n",
      "==================================================\n",
      "Question: Сколько дней отпуска в год я могу взять?\n",
      "==================================================\n",
      "Searching Consultant Plus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 18:30:56,242 - src.data.consultant_plus_loader - INFO - Successfully loaded document 1\n",
      "2025-11-30 18:30:57,744 - src.data.consultant_plus_loader - INFO - Loading document 2/4: При проверке личных карточек по форме N Т-2 у работника выявился неиспользованны...\n",
      "2025-11-30 18:30:57,796 - src.data.consultant_plus_loader - INFO - Successfully loaded document 2\n",
      "2025-11-30 18:30:59,298 - src.data.consultant_plus_loader - INFO - Loading document 3/4: Работница работает в организации с 15.03.2009, в середине июля уходит в декретны...\n",
      "2025-11-30 18:30:59,351 - src.data.consultant_plus_loader - INFO - Successfully loaded document 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrun_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mСколько дней отпуска в год я могу взять?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Документы\\MIPT\\law_rag\\scripts\\run_pipeline.py:34\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(questions)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 1. Load documents from Consultant Plus based on question\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSearching Consultant Plus...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m documents = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_documents_from_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m documents:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo documents found for this query\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Документы\\MIPT\\law_rag\\src\\data\\document_loader.py:54\u001b[39m, in \u001b[36mDocumentLoader.load_documents_from_query\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_documents_from_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> List[Document]:\n\u001b[32m     53\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convenience method to load documents from Consultant Plus using query\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconsultant_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Документы\\MIPT\\law_rag\\src\\data\\consultant_plus_loader.py:224\u001b[39m, in \u001b[36mConsultantPlusLoader.load_documents\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    221\u001b[39m         logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to load content for document \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# Be respectful with requests\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents from Consultant Plus\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m documents\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from scripts.run_pipeline import main\n",
    "main([\"трудовой кодекс отпуск\"])\n",
    "# main([\"Сколько дней отпуска в год я могу взять?\"])  # !!!!!!! добавить выделение ключевых слов для поиска по консультанту+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
