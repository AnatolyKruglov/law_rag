{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14331378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\\settings.py\n",
      "```\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "class Settings:\n",
      "    \"\"\"Configuration settings for the RAG pipeline\"\"\"\n",
      "    \n",
      "    # Yandex Cloud credentials\n",
      "    FOLDER_ID = os.getenv(\"YANDEX_FOLDER_ID\")\n",
      "    API_KEY = os.getenv(\"YANDEX_API_KEY\")\n",
      "    \n",
      "    # Document processing\n",
      "    CHUNK_SIZE = 1500\n",
      "    CHUNK_OVERLAP = 250\n",
      "    \n",
      "    # Model settings\n",
      "    TEMPERATURE = 0.3\n",
      "    MAX_TOKENS = 8000\n",
      "\n",
      "    # Retrieval settings\n",
      "    SEARCH_KWARGS = {\"k\": 5}  # Number of documents to retrieve\n",
      "    SCORE_THRESHOLD = 0.7  # Minimum similarity score\n",
      "\n",
      "settings = Settings()\n",
      "```\n",
      "\n",
      "config\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "scripts\\run_pipeline.py\n",
      "```\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Main script to run the RAG pipeline\n",
      "\"\"\"\n",
      "\n",
      "import sys\n",
      "import os\n",
      "import time\n",
      "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
      "\n",
      "from src.data.document_loader import DocumentLoader\n",
      "from src.processing.text_splitter import TextSplitter\n",
      "from src.processing.embeddings import EmbeddingManager\n",
      "from src.retrieval.vector_store import VectorStoreManager\n",
      "from src.generation.qa_chain import QASystem\n",
      "from src.utils.helpers import setup_logging, format_sources\n",
      "from config.settings import settings\n",
      "\n",
      "def main(questions):\n",
      "    setup_logging()\n",
      "    \n",
      "    try:\n",
      "        # Initialize document loader for Consultant Plus\n",
      "        print(\"Initializing Consultant Plus loader...\")\n",
      "        loader = DocumentLoader(use_consultant_plus=True)\n",
      "        \n",
      "        for question in questions:\n",
      "            print(f\"\\n{'='*50}\")\n",
      "            print(f\"Question: {question}\")\n",
      "            print(f\"{'='*50}\")\n",
      "            \n",
      "            # 1. Load documents from Consultant Plus based on question\n",
      "            print(\"Searching Consultant Plus...\")\n",
      "            documents = loader.load_documents_from_query(question)\n",
      "            \n",
      "            if not documents:\n",
      "                print(\"No documents found for this query\")\n",
      "                continue\n",
      "                \n",
      "            print(f\"Found {len(documents)} relevant documents\")\n",
      "            \n",
      "            # 2. Split documents\n",
      "            print(\"Processing documents...\")\n",
      "            splitter = TextSplitter()\n",
      "            chunks = splitter.split_documents(documents)\n",
      "            print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
      "            \n",
      "            # 3. Create embeddings and vector store with retry logic\n",
      "            print(\"Creating embeddings and vector store...\")\n",
      "            embedding_manager = EmbeddingManager()\n",
      "            embeddings = embedding_manager.get_embeddings()\n",
      "            \n",
      "            vector_manager = VectorStoreManager(embeddings)\n",
      "            \n",
      "            # Add retry logic for vector store creation\n",
      "            max_retries = 3\n",
      "            for attempt in range(max_retries):\n",
      "                try:\n",
      "                    vector_store = vector_manager.create_vector_store(chunks, batch_size=3)  # Small batch size\n",
      "                    break\n",
      "                except Exception as e:\n",
      "                    if \"rate quota limit exceed\" in str(e) and attempt < max_retries - 1:\n",
      "                        wait_time = (attempt + 1) * 15  # 15, 30, 45 seconds\n",
      "                        print(f\"Rate limit hit, waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n",
      "                        time.sleep(wait_time)\n",
      "                        continue\n",
      "                    else:\n",
      "                        raise\n",
      "            \n",
      "            # 4. Create QA system\n",
      "            print(\"Initializing QA system...\")\n",
      "            retriever = vector_manager.get_retriever()\n",
      "            qa_system = QASystem(retriever)\n",
      "            \n",
      "            # 5. Make query\n",
      "            # system_prompt = \"Ты специалист по российскому праву. Подробно объясни ответ, указав ссылки на источники\"\n",
      "            system_prompt = \"Ты специалист по российскому праву. Твоя задача: ответить на вопрос юзера, подробно объяснить ответ, дать пояснения простым языком (при необходимости - предоставить понятный алгоритм действий). Обязательно указать ссылки на источники\"\n",
      "\n",
      "            \n",
      "            result = qa_system.query(question, system_prompt)\n",
      "            print(f\"Answer: {result['answer']}\")\n",
      "            print(f\"\\nSources:\\n{format_sources(result['source_documents'])}\")\n",
      "            \n",
      "    except Exception as e:\n",
      "        print(f\"Error in pipeline: {e}\")\n",
      "        raise\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main(\n",
      "        questions=[\n",
      "            \"трудовой кодекс отпуск\",\n",
      "        ],\n",
      "    )\n",
      "```\n",
      "\n",
      "src\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "src\\data\\consultant_plus_loader.py\n",
      "```\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "from langchain.schema import Document\n",
      "from typing import List, Optional\n",
      "import urllib.parse\n",
      "import logging\n",
      "import time\n",
      "import re\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "class ConsultantPlusLoader:\n",
      "    \"\"\"Loader for Консультант Плюс search results and document content\"\"\"\n",
      "    \n",
      "    def __init__(self, max_results: int = 5):\n",
      "        self.max_results = max_results\n",
      "        self.base_url = \"https://www.consultant.ru\"\n",
      "        self.search_url = \"https://www.consultant.ru/search/\"\n",
      "        self.session = requests.Session()\n",
      "        self.session.headers.update({\n",
      "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
      "        })\n",
      "    \n",
      "    def search_documents(self, query: str) -> List[dict]:\n",
      "        \"\"\"Search documents on Консультант Плюс and return results with metadata\"\"\"\n",
      "        try:\n",
      "            encoded_query = urllib.parse.quote(query.encode('utf-8'))\n",
      "            url = f\"{self.search_url}?q={encoded_query}\"\n",
      "            \n",
      "            logger.info(f\"Fetching search results from: {url}\")\n",
      "            response = self.session.get(url, timeout=30)\n",
      "            response.raise_for_status()\n",
      "            \n",
      "            # Use lxml parser if available, otherwise use html.parser\n",
      "            try:\n",
      "                soup = BeautifulSoup(response.content, 'lxml')\n",
      "            except:\n",
      "                soup = BeautifulSoup(response.content, 'html.parser')\n",
      "            \n",
      "            results = []\n",
      "            \n",
      "            # Find search results\n",
      "            search_results = soup.find('ol', class_='search-results')\n",
      "            if not search_results:\n",
      "                logger.warning(\"No search results found on page\")\n",
      "                return results\n",
      "            \n",
      "            items = search_results.find_all('li', class_=re.compile('search-results__item'))[:self.max_results]\n",
      "            logger.info(f\"Found {len(items)} search result items\")\n",
      "            \n",
      "            for i, item in enumerate(items):\n",
      "                try:\n",
      "                    # Skip revoked documents and unavailable ones\n",
      "                    item_classes = item.get('class', [])\n",
      "                    if 'search-results__item_revoke' in item_classes:\n",
      "                        logger.debug(f\"Skipping revoked document at position {i}\")\n",
      "                        continue\n",
      "                    \n",
      "                    # Check availability\n",
      "                    availability_icon = item.find('i', class_='search-results__icon')\n",
      "                    if availability_icon and 'недоступен' in availability_icon.get('title', ''):\n",
      "                        logger.debug(f\"Skipping unavailable document at position {i}\")\n",
      "                        continue\n",
      "                    \n",
      "                    link = item.find('a', class_='search-results__link')\n",
      "                    if not link:\n",
      "                        continue\n",
      "                    \n",
      "                    href = link.get('href')\n",
      "                    if not href:\n",
      "                        continue\n",
      "                    \n",
      "                    # Make absolute URL\n",
      "                    if href.startswith('//'):\n",
      "                        doc_url = 'https:' + href\n",
      "                    elif href.startswith('/'):\n",
      "                        doc_url = self.base_url + href\n",
      "                    else:\n",
      "                        doc_url = href\n",
      "                    \n",
      "                    # Extract title\n",
      "                    title_elem = link.find('p', class_='search-results__link-inherit')\n",
      "                    title = title_elem.get_text(strip=True) if title_elem else \"No title\"\n",
      "                    \n",
      "                    # Extract description\n",
      "                    desc_elem = link.find('p', class_='search-results__descr')\n",
      "                    description = desc_elem.get_text(strip=True) if desc_elem else \"\"\n",
      "                    \n",
      "                    # Extract text info\n",
      "                    text_elem = link.find('p', class_='search-results__text')\n",
      "                    text_info = text_elem.get_text(strip=True) if text_elem else \"\"\n",
      "                    \n",
      "                    results.append({\n",
      "                        'url': doc_url,\n",
      "                        'title': title,\n",
      "                        'description': description,\n",
      "                        'text_info': text_info,\n",
      "                        'relevance_score': len(results) + 1,\n",
      "                        'position': i + 1\n",
      "                    })\n",
      "                    \n",
      "                    logger.debug(f\"Added result: {title[:50]}...\")\n",
      "                    \n",
      "                except Exception as e:\n",
      "                    logger.warning(f\"Error processing search result {i}: {e}\")\n",
      "                    continue\n",
      "            \n",
      "            logger.info(f\"Processed {len(results)} valid search results\")\n",
      "            return results\n",
      "            \n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error searching Consultant Plus: {e}\")\n",
      "            return []\n",
      "    \n",
      "    def load_document_content(self, url: str) -> Optional[str]:\n",
      "        \"\"\"Load and extract text content from a document URL\"\"\"\n",
      "        try:\n",
      "            logger.debug(f\"Loading document content from: {url}\")\n",
      "            response = self.session.get(url, timeout=30)\n",
      "            response.raise_for_status()\n",
      "            \n",
      "            # Check if it's XML content\n",
      "            content_type = response.headers.get('content-type', '').lower()\n",
      "            if 'xml' in content_type or url.endswith('.cgi'):\n",
      "                # Use XML parser for XML content\n",
      "                try:\n",
      "                    soup = BeautifulSoup(response.content, 'xml')\n",
      "                except:\n",
      "                    soup = BeautifulSoup(response.content, 'lxml')\n",
      "            else:\n",
      "                # Use HTML parser for regular pages\n",
      "                try:\n",
      "                    soup = BeautifulSoup(response.content, 'lxml')\n",
      "                except:\n",
      "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
      "            \n",
      "            # Remove unwanted elements\n",
      "            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):\n",
      "                element.decompose()\n",
      "            \n",
      "            # Try to find main content areas - Консультант Плюс specific selectors\n",
      "            content_selectors = [\n",
      "                '.document-page__text',\n",
      "                '.text',\n",
      "                'article',\n",
      "                'main',\n",
      "                '.content',\n",
      "                '.document-text',\n",
      "                '#content',\n",
      "                '.law-content'\n",
      "            ]\n",
      "            \n",
      "            content = None\n",
      "            for selector in content_selectors:\n",
      "                content_elem = soup.select_one(selector)\n",
      "                if content_elem:\n",
      "                    content = content_elem.get_text(separator=' ', strip=True)\n",
      "                    if len(content) > 100:  # Only use if we got substantial content\n",
      "                        break\n",
      "            \n",
      "            # If no specific content found, get body text but clean it up\n",
      "            if not content or len(content) < 100:\n",
      "                body = soup.find('body')\n",
      "                if body:\n",
      "                    # Remove empty elements and navigation\n",
      "                    for element in body.find_all(['div', 'span', 'p']):\n",
      "                        if len(element.get_text(strip=True)) < 10:\n",
      "                            element.decompose()\n",
      "                    content = body.get_text(separator=' ', strip=True)\n",
      "            \n",
      "            # Clean up the text - remove extra whitespace\n",
      "            if content:\n",
      "                content = re.sub(r'\\s+', ' ', content)\n",
      "                content = content.strip()\n",
      "                \n",
      "                # Basic validation - if content is too short, it might be an error page\n",
      "                if len(content) < 50:\n",
      "                    logger.warning(f\"Document content too short ({len(content)} chars), may be invalid\")\n",
      "                    return None\n",
      "                \n",
      "                logger.debug(f\"Extracted {len(content)} characters from document\")\n",
      "            \n",
      "            return content\n",
      "            \n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error loading document from {url}: {e}\")\n",
      "            return None\n",
      "    \n",
      "    def load_documents(self, query: str) -> List[Document]:\n",
      "        \"\"\"Main method to load documents based on search query\"\"\"\n",
      "        logger.info(f\"Searching Consultant Plus for: {query}\")\n",
      "        \n",
      "        search_results = self.search_documents(query)\n",
      "        if not search_results:\n",
      "            logger.warning(\"No search results found\")\n",
      "            return []\n",
      "        \n",
      "        documents = []\n",
      "        for i, result in enumerate(search_results):\n",
      "            logger.info(f\"Loading document {i+1}/{len(search_results)}: {result['title'][:80]}...\")\n",
      "            \n",
      "            content = self.load_document_content(result['url'])\n",
      "            if content:\n",
      "                # Create LangChain Document with metadata\n",
      "                metadata = {\n",
      "                    'source': result['url'],\n",
      "                    'title': result['title'],\n",
      "                    'description': result['description'],\n",
      "                    'text_info': result['text_info'],\n",
      "                    'relevance_score': result['relevance_score'],\n",
      "                    'position': result['position']\n",
      "                }\n",
      "                \n",
      "                document = Document(\n",
      "                    page_content=content,\n",
      "                    metadata=metadata\n",
      "                )\n",
      "                documents.append(document)\n",
      "                logger.info(f\"Successfully loaded document {i+1}\")\n",
      "            else:\n",
      "                logger.warning(f\"Failed to load content for document {i+1}\")\n",
      "            \n",
      "            # Be respectful with requests\n",
      "            time.sleep(1.5)\n",
      "        \n",
      "        logger.info(f\"Loaded {len(documents)} documents from Consultant Plus\")\n",
      "        return documents\n",
      "```\n",
      "\n",
      "src\\data\\document_loader.py\n",
      "```\n",
      "import os\n",
      "from typing import List\n",
      "from langchain.schema import Document\n",
      "\n",
      "# PDF functionality (commented but kept)\n",
      "\"\"\"\n",
      "from langchain_community.document_loaders import (\n",
      "    PyPDFLoader,\n",
      "    DirectoryLoader,\n",
      "    TextLoader,\n",
      "    UnstructuredFileLoader\n",
      ")\n",
      "\"\"\"\n",
      "\n",
      "from .consultant_plus_loader import ConsultantPlusLoader\n",
      "\n",
      "class DocumentLoader:\n",
      "    \"\"\"Main document loader that can use both PDFs and Consultant Plus\"\"\"\n",
      "    \n",
      "    def __init__(self, source: str = None, use_consultant_plus: bool = True):\n",
      "        self.source = source\n",
      "        self.use_consultant_plus = use_consultant_plus\n",
      "        self.consultant_loader = ConsultantPlusLoader()\n",
      "    \n",
      "    def load_documents(self, query: str = None) -> List[Document]:\n",
      "        \"\"\"Load documents from Consultant Plus based on query\"\"\"\n",
      "        if self.use_consultant_plus and query:\n",
      "            return self.consultant_loader.load_documents(query)\n",
      "        \n",
      "        # PDF functionality (commented but kept for reference)\n",
      "        \"\"\"\n",
      "        if not self.source or not os.path.exists(self.source):\n",
      "            raise ValueError(f\"Document path does not exist: {self.source}\")\n",
      "        \n",
      "        if os.path.isdir(self.source):\n",
      "            loader = DirectoryLoader(\n",
      "                self.source,\n",
      "                glob=\"**/*.pdf\",\n",
      "                loader_cls=PyPDFLoader\n",
      "            )\n",
      "        else:\n",
      "            if self.source.endswith('.pdf'):\n",
      "                loader = PyPDFLoader(self.source)\n",
      "            else:\n",
      "                loader = UnstructuredFileLoader(self.source)\n",
      "        \n",
      "        return loader.load()\n",
      "        \"\"\"\n",
      "        \n",
      "        raise ValueError(\"Either provide a query for Consultant Plus or enable PDF loading with valid source\")\n",
      "\n",
      "    def load_documents_from_query(self, query: str) -> List[Document]:\n",
      "        \"\"\"Convenience method to load documents from Consultant Plus using query\"\"\"\n",
      "        return self.consultant_loader.load_documents(query)\n",
      "```\n",
      "\n",
      "src\\data\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "src\\generation\\qa_chain.py\n",
      "```\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain_community.llms import YandexGPT\n",
      "from config.settings import settings\n",
      "import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "class QASystem:\n",
      "    \"\"\"Question-Answering system with RAG\"\"\"\n",
      "    \n",
      "    def __init__(self, retriever, llm=None):\n",
      "        self.retriever = retriever\n",
      "        self.llm = llm or self._create_llm()\n",
      "        self.qa_chain = self._create_qa_chain()\n",
      "    \n",
      "    def _create_llm(self):\n",
      "        \"\"\"Create YandexGPT LLM instance\"\"\"\n",
      "        return YandexGPT(\n",
      "            folder_id=settings.FOLDER_ID,\n",
      "            api_key=settings.API_KEY,\n",
      "            temperature=settings.TEMPERATURE,\n",
      "            max_tokens=settings.MAX_TOKENS\n",
      "        )\n",
      "    \n",
      "    def _create_qa_chain(self):\n",
      "        \"\"\"Create QA chain with custom prompt\"\"\"\n",
      "        prompt_template = \"\"\"Ты специалист по российскому праву. \n",
      "Используй предоставленный контекст из системы Консультант Плюс, чтобы подробно ответить на вопрос. \n",
      "Если в контексте нет достаточной информации, используй свои знания, но укажи это.\n",
      "\n",
      "Контекст:\n",
      "{context}\n",
      "\n",
      "Вопрос: {question}\n",
      "\n",
      "Требования к ответу:\n",
      "1. Подробно объясни ответ со ссылками на законодательство\n",
      "2. Укажи конкретные статьи и нормативные акты\n",
      "3. Будь точным и структурированным\n",
      "4. Укажи источники информации\n",
      "\n",
      "Ответ:\"\"\"\n",
      "        \n",
      "        PROMPT = PromptTemplate(\n",
      "            template=prompt_template, input_variables=[\"context\", \"question\"]\n",
      "        )\n",
      "        \n",
      "        return RetrievalQA.from_chain_type(\n",
      "            llm=self.llm,\n",
      "            chain_type=\"stuff\",\n",
      "            retriever=self.retriever,\n",
      "            chain_type_kwargs={\"prompt\": PROMPT},\n",
      "            return_source_documents=True\n",
      "        )\n",
      "    \n",
      "    def query(self, question: str, system_prompt: str = None) -> dict:\n",
      "        \"\"\"Query the QA system\"\"\"\n",
      "        try:\n",
      "            if system_prompt:\n",
      "                full_question = f\"{system_prompt}\\n\\nВопрос: {question}\"\n",
      "            else:\n",
      "                full_question = question\n",
      "            \n",
      "            result = self.qa_chain({\"query\": full_question})\n",
      "            return {\n",
      "                \"answer\": result[\"result\"],\n",
      "                \"source_documents\": result.get(\"source_documents\", []),\n",
      "                \"question\": question\n",
      "            }\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error querying QA system: {e}\")\n",
      "            raise\n",
      "```\n",
      "\n",
      "src\\generation\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "src\\processing\\embeddings.py\n",
      "```\n",
      "from langchain_community.embeddings.yandex import YandexGPTEmbeddings\n",
      "from config.settings import settings\n",
      "import logging\n",
      "import time\n",
      "import threading\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "class EmbeddingManager:\n",
      "    \"\"\"Manages embedding generation with aggressive rate limiting\"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.embeddings = YandexGPTEmbeddings(\n",
      "            folder_id=settings.FOLDER_ID,\n",
      "            api_key=settings.API_KEY\n",
      "        )\n",
      "        self.last_request_time = 0\n",
      "        self.min_interval = 0.5  # Increased to 500ms between requests (2 requests per second)\n",
      "        self.lock = threading.Lock()\n",
      "    \n",
      "    def get_embeddings(self):\n",
      "        \"\"\"Get embeddings instance\"\"\"\n",
      "        return self.embeddings\n",
      "    \n",
      "    def _rate_limited_embed(self, texts):\n",
      "        \"\"\"Apply rate limiting to embedding requests\"\"\"\n",
      "        with self.lock:\n",
      "            current_time = time.time()\n",
      "            time_since_last = current_time - self.last_request_time\n",
      "            if time_since_last < self.min_interval:\n",
      "                sleep_time = self.min_interval - time_since_last\n",
      "                logger.info(f\"Rate limiting: sleeping for {sleep_time:.2f}s\")\n",
      "                time.sleep(sleep_time)\n",
      "            \n",
      "            self.last_request_time = time.time()\n",
      "            return self.embeddings.embed_documents(texts)\n",
      "    \n",
      "    def embed_documents(self, documents: list):\n",
      "        \"\"\"Embed a list of documents with rate limiting\"\"\"\n",
      "        try:\n",
      "            texts = [doc.page_content for doc in documents]\n",
      "            \n",
      "            # Process in even smaller batches\n",
      "            batch_size = 3  # Further reduced batch size\n",
      "            all_embeddings = []\n",
      "            \n",
      "            for i in range(0, len(texts), batch_size):\n",
      "                batch_texts = texts[i:i + batch_size]\n",
      "                batch_num = (i // batch_size) + 1\n",
      "                total_batches = (len(texts) - 1) // batch_size + 1\n",
      "                logger.info(f\"Embedding batch {batch_num}/{total_batches} ({len(batch_texts)} texts)\")\n",
      "                \n",
      "                batch_embeddings = self._rate_limited_embed(batch_texts)\n",
      "                all_embeddings.extend(batch_embeddings)\n",
      "                \n",
      "                # Longer delay between batches\n",
      "                if i + batch_size < len(texts):\n",
      "                    logger.info(\"Sleeping between batches...\")\n",
      "                    time.sleep(1.5)  # Increased delay\n",
      "            \n",
      "            return all_embeddings\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error embedding documents: {e}\")\n",
      "            raise e\n",
      "```\n",
      "\n",
      "src\\processing\\text_splitter.py\n",
      "```\n",
      "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
      "from langchain.schema import Document\n",
      "from typing import List\n",
      "from config.settings import settings\n",
      "\n",
      "class TextSplitter:\n",
      "    \"\"\"Handles document splitting with various strategies\"\"\"\n",
      "    \n",
      "    def __init__(self, chunk_size: int = None, chunk_overlap: int = None):\n",
      "        self.chunk_size = chunk_size or settings.CHUNK_SIZE\n",
      "        self.chunk_overlap = chunk_overlap or settings.CHUNK_OVERLAP\n",
      "    \n",
      "    def split_documents(self, documents: List[Document], method: str = \"recursive\") -> List[Document]:\n",
      "        \"\"\"Split documents into chunks\"\"\"\n",
      "        if method == \"recursive\":\n",
      "            splitter = RecursiveCharacterTextSplitter(\n",
      "                chunk_size=self.chunk_size,\n",
      "                chunk_overlap=self.chunk_overlap,\n",
      "                separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
      "            )\n",
      "        else:\n",
      "            splitter = CharacterTextSplitter(\n",
      "                chunk_size=self.chunk_size,\n",
      "                chunk_overlap=self.chunk_overlap\n",
      "            )\n",
      "        \n",
      "        return splitter.split_documents(documents)\n",
      "```\n",
      "\n",
      "src\\processing\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "src\\retrieval\\retriever.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "src\\retrieval\\vector_store.py\n",
      "```\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.schema import Document\n",
      "from typing import List, Optional\n",
      "import logging\n",
      "from config.settings import settings\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "class VectorStoreManager:\n",
      "    \"\"\"Manages FAISS vector store operations with optimized batch processing\"\"\"\n",
      "    \n",
      "    def __init__(self, embeddings):\n",
      "        self.embeddings = embeddings\n",
      "        self.vector_store = None\n",
      "    \n",
      "    def create_vector_store(self, documents: List[Document], batch_size: int = 5):  # Reduced batch size\n",
      "        \"\"\"Create FAISS vector store from documents with batching\"\"\"\n",
      "        try:\n",
      "            if len(documents) > batch_size:\n",
      "                # Process in smaller batches to avoid rate limits\n",
      "                logger.info(f\"Creating vector store with {len(documents)} documents in batches of {batch_size}\")\n",
      "                \n",
      "                # Initialize with first batch\n",
      "                first_batch = documents[:batch_size]\n",
      "                self.vector_store = FAISS.from_documents(first_batch, self.embeddings)\n",
      "                logger.info(f\"Initialized vector store with first {len(first_batch)} documents\")\n",
      "                \n",
      "                # Add remaining documents in batches\n",
      "                for i in range(batch_size, len(documents), batch_size):\n",
      "                    batch = documents[i:i + batch_size]\n",
      "                    batch_num = (i // batch_size) + 1\n",
      "                    total_batches = (len(documents) - 1) // batch_size + 1\n",
      "                    logger.info(f\"Adding batch {batch_num}/{total_batches} ({len(batch)} documents)\")\n",
      "                    \n",
      "                    self.vector_store.add_documents(batch)\n",
      "                    \n",
      "                    # More aggressive rate limiting between batches\n",
      "                    if i + batch_size < len(documents):\n",
      "                        import time\n",
      "                        time.sleep(2)  # Increased to 2 seconds between batches\n",
      "            else:\n",
      "                self.vector_store = FAISS.from_documents(documents, self.embeddings)\n",
      "            \n",
      "            logger.info(f\"Created vector store with {len(documents)} documents\")\n",
      "            return self.vector_store\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error creating vector store: {e}\")\n",
      "            raise\n",
      "    \n",
      "    def save_vector_store(self, path: str):\n",
      "        \"\"\"Save vector store to disk\"\"\"\n",
      "        if self.vector_store:\n",
      "            self.vector_store.save_local(path)\n",
      "            logger.info(f\"Vector store saved to {path}\")\n",
      "    \n",
      "    def load_vector_store(self, path: str):\n",
      "        \"\"\"Load vector store from disk\"\"\"\n",
      "        try:\n",
      "            self.vector_store = FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization=True)\n",
      "            logger.info(f\"Vector store loaded from {path}\")\n",
      "            return self.vector_store\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error loading vector store: {e}\")\n",
      "            raise\n",
      "    \n",
      "    def get_retriever(self, search_type: str = \"similarity\", **kwargs):\n",
      "        \"\"\"Get retriever from vector store\"\"\"\n",
      "        if not self.vector_store:\n",
      "            raise ValueError(\"Vector store not initialized\")\n",
      "        \n",
      "        search_kwargs = {**settings.SEARCH_KWARGS, **kwargs}\n",
      "        return self.vector_store.as_retriever(\n",
      "            search_type=search_type,\n",
      "            search_kwargs=search_kwargs\n",
      "        )\n",
      "```\n",
      "\n",
      "src\\retrieval\\vector_store_optimized\n",
      "```\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.schema import Document\n",
      "from typing import List\n",
      "import logging\n",
      "import time\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "class OptimizedVectorStoreManager:\n",
      "    \"\"\"Optimized vector store manager with very conservative rate limiting\"\"\"\n",
      "    \n",
      "    def __init__(self, embeddings):\n",
      "        self.embeddings = embeddings\n",
      "        self.vector_store = None\n",
      "    \n",
      "    def create_vector_store(self, documents: List[Document]):\n",
      "        \"\"\"Create vector store with very conservative settings\"\"\"\n",
      "        try:\n",
      "            logger.info(f\"Creating vector store with {len(documents)} documents\")\n",
      "            \n",
      "            # Use FAISS's built-in batch processing with custom rate limiting\n",
      "            self.vector_store = FAISS.from_documents(\n",
      "                documents, \n",
      "                self.embeddings,\n",
      "                batch_size=2,  # Very small batch size\n",
      "                rate_limiter={\n",
      "                    'requests_per_second': 2,  # Very conservative\n",
      "                    'batch_wait_time': 1.0\n",
      "                }\n",
      "            )\n",
      "            \n",
      "            logger.info(\"Vector store created successfully\")\n",
      "            return self.vector_store\n",
      "            \n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error creating vector store: {e}\")\n",
      "            # Fallback: try one document at a time\n",
      "            return self._create_slow_vector_store(documents)\n",
      "    \n",
      "    def _create_slow_vector_store(self, documents: List[Document]):\n",
      "        \"\"\"Fallback method - process one document at a time\"\"\"\n",
      "        logger.info(\"Using fallback slow vector store creation\")\n",
      "        \n",
      "        if not documents:\n",
      "            raise ValueError(\"No documents to process\")\n",
      "        \n",
      "        # Start with first document\n",
      "        self.vector_store = FAISS.from_documents([documents[0]], self.embeddings)\n",
      "        logger.info(f\"Added 1/{len(documents)} documents\")\n",
      "        \n",
      "        # Add remaining documents one by one with delays\n",
      "        for i in range(1, len(documents)):\n",
      "            time.sleep(2)  # 2 second delay between each document\n",
      "            self.vector_store.add_documents([documents[i]])\n",
      "            logger.info(f\"Added {i+1}/{len(documents)} documents\")\n",
      "        \n",
      "        return self.vector_store\n",
      "    \n",
      "    def get_retriever(self, search_type: str = \"similarity\", **kwargs):\n",
      "        \"\"\"Get retriever from vector store\"\"\"\n",
      "        if not self.vector_store:\n",
      "            raise ValueError(\"Vector store not initialized\")\n",
      "        \n",
      "        from config.settings import settings\n",
      "        search_kwargs = {**settings.SEARCH_KWARGS, **kwargs}\n",
      "        return self.vector_store.as_retriever(\n",
      "            search_type=search_type,\n",
      "            search_kwargs=search_kwargs\n",
      "        )\n",
      "```\n",
      "\n",
      "src\\retrieval\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "src\\utils\\helpers.py\n",
      "```\n",
      "import logging\n",
      "from typing import List\n",
      "\n",
      "def setup_logging(level=logging.INFO):\n",
      "    \"\"\"Setup logging configuration\"\"\"\n",
      "    logging.basicConfig(\n",
      "        level=level,\n",
      "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
      "        handlers=[\n",
      "            logging.StreamHandler(),\n",
      "            logging.FileHandler('rag_pipeline.log')\n",
      "        ]\n",
      "    )\n",
      "\n",
      "def format_sources(source_documents: List) -> str:\n",
      "    \"\"\"Format source documents for display\"\"\"\n",
      "    if not source_documents:\n",
      "        return \"No sources found\"\n",
      "    \n",
      "    sources = []\n",
      "    for i, doc in enumerate(source_documents, 1):\n",
      "        source_info = f\"Source {i}:\"\n",
      "        if hasattr(doc, 'metadata') and 'source' in doc.metadata:\n",
      "            source_info += f\" {doc.metadata['source']}\"\n",
      "        if hasattr(doc, 'metadata') and 'page' in doc.metadata:\n",
      "            source_info += f\" (Page {doc.metadata['page']})\"\n",
      "        sources.append(source_info)\n",
      "    \n",
      "    return \"\\n\".join(sources)\n",
      "```\n",
      "\n",
      "src\\utils\\__init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def print_repo_contents(repo_path=\".\"):\n",
    "    \"\"\"Print all file contents in the repository\"\"\"\n",
    "    repo_path = Path(repo_path)\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            if file.startswith('.') or file.endswith('.pyc') or file.endswith('.md') or file.endswith('.txt') or file.endswith('.ipynb') or file.endswith('.log'):\n",
    "                continue\n",
    "            print(f\"{file_path}\")\n",
    "            print(\"```\")\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                print(f.read())\n",
    "            print(\"```\\n\")\n",
    "\n",
    "print_repo_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db3298c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 20:47:03,432 - src.data.consultant_plus_loader - INFO - Searching Consultant Plus for: 'Сколько налогов я заплачу в год при зарплате 2500000 в год?'\n",
      "2025-11-30 20:47:03,433 - src.data.consultant_plus_loader - INFO - Reformulating query: 'Сколько налогов я заплачу в год при зарплате 2500000 в год?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Consultant Plus loader...\n",
      "\n",
      "==================================================\n",
      "Question: Сколько налогов я заплачу в год при зарплате 2500000 в год?\n",
      "==================================================\n",
      "Searching Consultant Plus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 20:47:03,905 - src.processing.query_reformulator - INFO - Query reformulated: 'Сколько налогов я заплачу в год при зарплате 2500000 в год?' -> '\"налоговый кодекс ставка ндфл\"'\n",
      "2025-11-30 20:47:03,907 - src.data.consultant_plus_loader - INFO - Reformulated query: '\"налоговый кодекс ставка ндфл\"'\n",
      "2025-11-30 20:47:03,908 - src.data.consultant_plus_loader - INFO - Fetching search results from: https://www.consultant.ru/search/?q=%22%D0%BD%D0%B0%D0%BB%D0%BE%D0%B3%D0%BE%D0%B2%D1%8B%D0%B9%20%D0%BA%D0%BE%D0%B4%D0%B5%D0%BA%D1%81%20%D1%81%D1%82%D0%B0%D0%B2%D0%BA%D0%B0%20%D0%BD%D0%B4%D1%84%D0%BB%22\n",
      "2025-11-30 20:47:04,005 - src.data.consultant_plus_loader - INFO - Found 5 search result items\n",
      "2025-11-30 20:47:04,007 - src.data.consultant_plus_loader - INFO - Processed 4 valid search results\n",
      "2025-11-30 20:47:04,007 - src.data.consultant_plus_loader - INFO - Loading document 1/4: \"НалоговыйкодексРоссийской Федерации (часть вторая)\" от 05.08.2000 N 117-ФЗ...\n",
      "2025-11-30 20:47:04,465 - src.data.consultant_plus_loader - INFO - Successfully loaded document 1\n",
      "2025-11-30 20:47:05,966 - src.data.consultant_plus_loader - INFO - Loading document 2/4: \"О внесении изменений в статью 224 части второйНалоговогокодексаРоссийской Федер...\n",
      "2025-11-30 20:47:06,063 - src.data.consultant_plus_loader - INFO - Successfully loaded document 2\n",
      "2025-11-30 20:47:07,565 - src.data.consultant_plus_loader - INFO - Loading document 3/4: \"НалоговыйкодексРоссийской Федерации (часть первая)\" от 31.07.1998 N 146-ФЗ...\n",
      "2025-11-30 20:47:07,759 - src.data.consultant_plus_loader - INFO - Successfully loaded document 3\n",
      "2025-11-30 20:47:09,260 - src.data.consultant_plus_loader - INFO - Loading document 4/4: Каков порядок исчисления и уплатыНДФЛс суммы неустойки, выплачиваемой застройщик...\n",
      "2025-11-30 20:47:09,301 - src.data.consultant_plus_loader - INFO - Successfully loaded document 4\n",
      "2025-11-30 20:47:10,803 - src.data.consultant_plus_loader - INFO - Loaded 4 documents from Consultant Plus\n",
      "2025-11-30 20:47:10,809 - src.retrieval.vector_store - INFO - Creating vector store with 98 documents in batches of 3\n",
      "2025-11-30 20:47:10,935 - src.retrieval.vector_store - INFO - Initialized vector store with first 3 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 relevant documents\n",
      "Processing documents...\n",
      "Created 98 chunks from 4 documents\n",
      "Creating embeddings and vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:07<00:00,  2.12s/it]\n",
      "2025-11-30 20:48:18,674 - src.retrieval.vector_store - INFO - Created vector store with 98 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing QA system...\n",
      "Answer: Для расчёта налогов, которые вы заплатите в год при зарплате 2 500 000 рублей, необходимо учесть налог на доходы физических лиц (НДФЛ). Согласно статье 224 Налогового кодекса Российской Федерации (НК РФ), основная ставка НДФЛ составляет 13% для доходов физических лиц — налоговых резидентов РФ.\n",
      "\n",
      "Расчёт будет следующим:\n",
      "\n",
      "$2 500 000 \\times 13\\% = 325 000$ рублей.\n",
      "\n",
      "Таким образом, при годовой зарплате в 2 500 000 рублей вы заплатите НДФЛ в размере 325 000 рублей.\n",
      "\n",
      "Источник: Налоговый кодекс Российской Федерации (часть вторая) от 05.08.2000 № 117-ФЗ (ред. от 25.12.2023) (с изм. и доп., вступ. в силу с 01.01.2024), статья 224.\n",
      "\n",
      "Sources:\n",
      "Source 1: https://www.consultant.ru/document/cons_doc_LAW_519034/\n",
      "Source 2: https://www.consultant.ru/document/cons_doc_LAW_519034/\n",
      "Source 3: https://www.consultant.ru/document/cons_doc_LAW_519034/\n",
      "Source 4: https://www.consultant.ru/document/cons_doc_LAW_519034/\n",
      "Source 5: https://www.consultant.ru/document/cons_doc_LAW_519034/\n"
     ]
    }
   ],
   "source": [
    "from scripts.run_pipeline import main\n",
    "# main([\"трудовой кодекс отпуск\"])\n",
    "# main([\"Сколько дней отпуска в год я могу взять?\"])\n",
    "main([\"Сколько налогов я заплачу в год при зарплате 2500000 в год?\"])  # почти всегда неправильно считает((("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
